{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#This section is for datacollection\n",
    "#The dataset is found here ---->   https://archive.ics.uci.edu/dataset/410/paper+reviews\n",
    "#The dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statistics\n",
    "file_path = 'dataset/reviews.json'\n",
    "\n",
    "\n",
    "decisionCounts = {\n",
    "    'accept': 0,\n",
    "    'probably reject': 0,\n",
    "    'reject': 0,\n",
    "    'no decision': 0\n",
    "}\n",
    "\n",
    "\n",
    "evaluations = []\n",
    "confidences = []\n",
    "orientations = []\n",
    "missingValuesPositions = []\n",
    "nullValuesCount= 0\n",
    "\n",
    "# Open the .JSON file \n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "for paper in data['paper']:\n",
    "    print(\"Paper ID:\", paper['id'])\n",
    "    decision = paper['preliminary_decision'].lower()  \n",
    "    print(\"Preliminary Decision:\", decision)\n",
    "\n",
    "\n",
    "    if decision in decisionCounts:\n",
    "        decisionCounts[decision] += 1\n",
    "    else:\n",
    "        \n",
    "        decisionCounts[decision] = 1\n",
    "\n",
    "    print(\"Reviews:\")\n",
    "    for review in paper['review']:\n",
    "        print(\"  Review ID:\", review['id'])\n",
    "        print(\"  Evaluation:\", review['evaluation'])\n",
    "        print(\"  Confidence:\", review['confidence'])\n",
    "        print(\"  Date:\", review['timespan'])\n",
    "        print(\"\")\n",
    "    print()  \n",
    "    \n",
    "\n",
    "evaluations = []\n",
    "confidences = []\n",
    "orientations = []\n",
    "decisions = []\n",
    "\n",
    "\n",
    "decisionMapping = {\n",
    "    \"no decision\": 0,  \n",
    "    \"accept\": 1,\n",
    "    \"probably reject\": 2,\n",
    "    \"reject\": 3\n",
    "}\n",
    "\n",
    "for paper in data['paper']:\n",
    "    for review in paper['review']:\n",
    "        if 'confidence' in review and review['confidence'] is not None:\n",
    "            confidences.append(int(review['confidence']))\n",
    "\n",
    "\n",
    "medianConfidence = np.median(confidences)  \n",
    "\n",
    "\n",
    "evaluations = []\n",
    "confidences = []\n",
    "orientations = []\n",
    "decisions = []\n",
    "\n",
    "for paper in data['paper']:\n",
    "    paperDecision = paper['preliminary_decision']\n",
    "    # Converting the decision, in text, into a numerical code\n",
    "    \n",
    "    decisionCode = decisionMapping.get(paperDecision, 0)  \n",
    "\n",
    "    for review in paper['review']:\n",
    "        if 'evaluation' in review and review['evaluation'] is not None:\n",
    "            evaluations.append(int(review['evaluation']))\n",
    "        else:\n",
    "            nullValuesCount += 1\n",
    "\n",
    "        if 'confidence' in review and review['confidence'] is not None:\n",
    "            confidences.append(int(review['confidence']))\n",
    "        else:\n",
    "            nullValuesCount += 1\n",
    "            confidences.append(int(medianConfidence))  # assuming medianConfidence is calculated elsewhere\n",
    "\n",
    "        if 'orientation' in review and review['orientation'] is not None:\n",
    "            orientations.append(int(review['orientation']))\n",
    "        else:\n",
    "            nullValuesCount += 1\n",
    "        \n",
    "        # Append the decision code for each review\n",
    "        decisions.append(decisionCode)\n",
    "\n",
    "for paper in data['paper']:\n",
    "    for review in paper['review']:\n",
    "        if 'decision' in review:\n",
    "            \n",
    "            decisionClass = review['decision']\n",
    "        else:\n",
    "            decisionClass = 0  \n",
    "\n",
    "       # decisions.append(decisionClass)\n",
    "\n",
    "print(f\"There are combined {nullValuesCount} null values for orientation, confidence and evaluation.\")\n",
    "\n",
    "evaluations_np = np.array(evaluations)\n",
    "\n",
    "\n",
    "\n",
    "confidencesArray = np.array(confidences)\n",
    "meanConfidence = np.mean(confidencesArray)\n",
    "medianConfidence = np.median(confidencesArray)\n",
    "\n",
    "print(f\"The length of 'confidences' is {len(confidences)}\")\n",
    "print(f\"The length of 'evaluations' is {len(evaluations)}\")\n",
    "print(int(medianConfidence))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c183b1a19eaa9b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Evaluation': evaluations,\n",
    "    'Confidence': confidences,\n",
    "    'Orientation': orientations,\n",
    "    'Decision': decisions\n",
    "})\n",
    "\n",
    "\n",
    "X = df[['Confidence', 'Orientation', 'Decision']]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "xScaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "kValues = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "silhouetteScores = []\n",
    "\n",
    "for cluster in kValues:\n",
    "    kmeans = KMeans(n_clusters=cluster, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(xScaled)\n",
    "    \n",
    "    silhouetteAvg = silhouette_score(xScaled, cluster_labels)\n",
    "    silhouetteScores.append(silhouette_avg)\n",
    "    \n",
    "    print(f\"The silhouette score when we have {cluster} clusters is {silhouetteAvg}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "661406af051ce6a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
